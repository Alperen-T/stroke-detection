{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Constants\nIMAGE_SIZE = (512, 512)\nWINDOW = {'level' : 45, 'width' : 90}\nDATASET_DIR = \"../input/stroke-dataset/\"\nINPUT_DIR = \"../input/stroke-dataset/DICOM/\"\nMASK_DIR = \"../input/stroke-dataset/MASK/\"\nTRAIN_DIR = \"../input/stroke-dataset/train_oversampled4x_v2.txt\"\nVALID_DIR = \"../input/stroke-dataset/valid_v2.txt\"\nBATCH_SIZE = 8\nEPOCH_COUNT = 100\nPROJECT_NAME = \"Stroke Detection\"\nLEARNING_RATE = 2.36e-3\nLEARNING_RATE_DECAY = 0.995\nTRAIN_BATCH_COUNT = 25\nVALID_BATCH_COUNT = 10\nVALID_IMAGE_COUNT = 50\nMEAN = 0.449\nSTD = 0.226\nSAVE_PER_BATCH = 2\nDRIVE_ID = \"\"\n\nVALID_IMAGE_NAMES =  [\"15525_Iskemi\", \"11858_Iskemi\", \"11689_Iskemi\", \"15726_Iskemi\", \"16511_Iskemi\", \"11314_Iskemi\", \"11655_Iskemi\", \"10937_Iskemi\", \"15677_Iskemi\", \"14055_Iskemi\", \"11159_Iskemi\", \"15125_Iskemi\", \"13343_Iskemi\", \"15029_Iskemi\", \"12878_Iskemi\", \"10907_Iskemi\", \"15853_Iskemi\", \"12677_Iskemi\", \"14575_Iskemi\", \"14792_Iskemi\", \"14297_Iskemi\", \"10442_Kanama\", \"10050_Kanama\", \"17011_Kanama\", \"10975_Kanama\", \"10876_Kanama\", \"15455_Kanama\", \"13385_Kanama\", \"14903_Kanama\", \"13411_Kanama\", \"10395_Kanama\", \"10549_Kanama\", \"16940_Kanama\", \"11728_Kanama\", \"15898_Kanama\", \"12312_Kanama\", \"15597_Kanama\", \"15338_Kanama\", \"13543_Kanama\", \"11049_Kanama\", \"15559_Kanama\", \"15788_Kanama\", \"14747_Kanama\", \"14623_Kanama\", \"10046_Kanama\", \"16658_InmeYok\", \"14596_InmeYok\", \"13093_InmeYok\", \"12410_InmeYok\", \"11388_InmeYok\"]\n\nimport torch\n\nif torch.cuda.is_available():  \n    DEVICE = 'cuda:0'\nelse:  \n    DEVICE = 'cpu'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_modality_lut\nimport gdcm\nimport cv2\n\ndef resize_image(image, new_size):\n    return cv2.resize(image, new_size)\n\ndef read_dicom(file_loc):\n    dicom = pydicom.dcmread(file_loc)\n    dicom.BitsStored = 16\n    return apply_modality_lut(dicom.pixel_array, dicom)\n\ndef window_dicom(image, window):\n    return (np.clip((image - (window['level'] - (window['width'] / 2))) / window['width'], 0., 1.0) * 255).astype('uint8')\n\ndef read_image(file_loc):\n    return cv2.imread(file_loc, cv2.IMREAD_GRAYSCALE)\n\nimport matplotlib.pyplot as plt\n\nplt.style.use('ggplot')\n\ndef show_image(image, color, show_now = True):\n    if(show_now):\n        plt.figure(figsize = (10, 10))\n        \n    if(color == 'gray'):\n        plt.imshow(image, cmap = plt.cm.gray, vmin = 0, vmax = 255)\n    elif(color == 'rgb'):\n        plt.imshow(image, vmin = 0, vmax = 255)\n        \n    plt.xticks(())\n    plt.yticks(())\n    \n    if(show_now):     \n        plt.show()\n        plt.close()\n\ndef show_all(images, titles):\n    col_cnt = len(images)\n    row_cnt = len(images[0])\n    \n    plt.figure(figsize = (col_cnt * 10, row_cnt * 10))\n    \n    for row in range(row_cnt):\n        for col in range(col_cnt):\n            if(len(images[col][row].shape) == 2):\n                plt.subplot(row_cnt, col_cnt, row * col_cnt + (col + 1))\n                show_image(images[col][row], 'gray', False)\n                plt.title(titles[col] + \" \" + str(row + 1))\n            elif(len(images[col][row].shape) == 3 and images[col][row].shape[-1] == 3):\n                plt.subplot(row_cnt, col_cnt, row * col_cnt + (col + 1))\n                show_image(images[col][row], 'rgb', False)\n                plt.title(titles[col] + \" \" + str(row + 1))\n            \n    plt.show()\n    plt.close()\n    \ndef fix_image_colors(image):\n    return image * 127.5\n\ndef get_difference(image, mask):\n    diffdict = {'diff' : np.zeros(image.shape + (3,))}\n    \n    diffdict['diff'][(image != mask) & (image != 0)] = [255, 0, 0]\n    diffdict['diff'][(image == mask) & (image != 0)] = [0, 255, 0]\n    diffdict['diff'][(image != mask) & (image == 0)] = [0, 0, 255]\n    \n    diffdict['red'] = int(((image != mask) & (image != 0)).sum())\n    diffdict['green'] = int(((image == mask) & (image != 0)).sum())\n    diffdict['blue'] = int(((image != mask) & (image == 0)).sum())\n    \n    return diffdict\n\nfrom segmentation_models_pytorch.encoders import preprocess_input\n\ndef preprocess(image):\n    return preprocess_input(image, mean = MEAN, std = STD, input_range = [0, 1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paths = {'train' : {}, 'valid' : {}, 'valid_images' : {}}\n\ntrain_names_file = open(TRAIN_DIR, \"r\")\ntrain_names = train_names_file.read().split('\\n')\n\npaths['train']['in'] = [INPUT_DIR + name + \".dcm\" for name in train_names]\npaths['train']['mask'] = [MASK_DIR + name + \".png\" for name in train_names]\n\nvalid_names_file = open(VALID_DIR, \"r\")\nvalid_names = valid_names_file.read().split('\\n')\n\npaths['valid']['in'] = [INPUT_DIR + name + \".dcm\" for name in valid_names]\npaths['valid']['mask'] = [MASK_DIR + name + \".png\" for name in valid_names]\n\npaths['valid_images']['in'] = [INPUT_DIR + name + \".dcm\" for name in VALID_IMAGE_NAMES]\npaths['valid_images']['mask'] = [MASK_DIR + name + \".png\" for name in VALID_IMAGE_NAMES]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import albumentations as A\n\nresize = A.Compose([\n    A.Resize(IMAGE_SIZE[0], IMAGE_SIZE[1], p = 1),\n])\n\ntransform = A.Compose([\n    A.HorizontalFlip(p = 0.5),\n#     A.CenterCrop(height = IMAGE_SIZE[0] // 2 + IMAGE_SIZE[0] // 3, width = IMAGE_SIZE[1] // 2 + IMAGE_SIZE[1] // 3, p = 0.2),\n    A.Resize(IMAGE_SIZE[0], IMAGE_SIZE[1], p = 1),\n#     A.Downscale(scale_min = 0.25, scale_max = 0.5, p = 0.1),\n    A.RandomSizedCrop((IMAGE_SIZE[0] // 2 + IMAGE_SIZE[0] // 4, IMAGE_SIZE[0]), height = IMAGE_SIZE[0], width = IMAGE_SIZE[1], p = 0.8),\n#     A.ElasticTransform(p = 0.8),\n#     A.GridDistortion(p = 0.9),\n    A.GaussNoise(p = 0.2),\n    A.Affine(shear = (-45, 45), p = 0.2),\n])\n\nfrom torch.utils.data import Dataset\n\nclass Stroke_DataSet(Dataset):\n    def __init__(self, input_paths, mask_paths, window, transform = None):\n        self.input_paths = input_paths\n        self.mask_paths = mask_paths\n        self.window = window\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.input_paths)\n    \n    def __getitem__(self, i):\n        dicom = read_dicom(self.input_paths[i])\n        dicom = window_dicom(dicom, self.window)\n        \n        mask = read_image(self.mask_paths[i])\n        \n        if(self.transform != None):\n            transformed = self.transform(image = dicom, mask = mask)\n            dicom = transformed['image']\n            mask = transformed['mask']\n            \n        dicom = dicom.astype('float32')\n        mask = mask.astype('int64')\n        \n        return (dicom, mask)\n    \ndatasets = {}\n\ndatasets['train'] = Stroke_DataSet(paths['train']['in'], paths['train']['mask'], WINDOW, transform)\ndatasets['valid'] = Stroke_DataSet(paths['valid']['in'], paths['valid']['mask'], WINDOW, resize)\ndatasets['valid_images'] = Stroke_DataSet(paths['valid_images']['in'], paths['valid_images']['mask'], WINDOW, resize)\n\nfrom torch.utils.data import DataLoader\n\nloaders = {}\n\nloaders['train'] = DataLoader(datasets['train'], batch_size = BATCH_SIZE, num_workers = 4, pin_memory = True)\nloaders['valid'] = DataLoader(datasets['valid'], batch_size = BATCH_SIZE, num_workers = 4, pin_memory = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport segmentation_models_pytorch as smp\nfrom google_drive_downloader import GoogleDriveDownloader as gdd\n\ntorch.cuda.empty_cache()\n\nis_saved = False\ndrive_id = DRIVE_ID\n\nif(is_saved):\n    gdd.download_file_from_google_drive(drive_id, \"./checkpoint.pth\")\n    checkpoint = torch.load(\"./checkpoint.pth\", map_location = DEVICE)\n    \n    model = checkpoint['model']\n    \n    # Combined olan icin:\n    DiceLoss = checkpoint['criterions'][0]\n    FocalLoss = checkpoint['criterions'][1]\n    NormFocalLoss = checkpoint['criterions'][2]\n    \n    optimizer = checkpoint['optimizer']\n    lr_scheduler = checkpoint['lr_scheduler']\n    \n    start_epoch = checkpoint['epoch'] + 1\n    ################################\n    \n#     criterion = checkpoint['criterion']\n#     criterion = smp.losses.FocalLoss(mode = 'multiclass')\n#     optimizer = checkpoint['optimizer']\n#     lr_scheduler = checkpoint['lr_scheduler']\n#     lr_scheduler = checkpoint['lr_sched']\n#     start_epoch = checkpoint['epoch'] + 1\n\nelse:\n#     model = smp.Unet(\n#         encoder_name = \"timm-efficientnet-b0\",\n#         encoder_weights = \"imagenet\",\n#         in_channels = 1,\n#         classes = 3,\n#         activation = None,\n# #         decoder_attention_type = \"scse\",\n#     )\n\n#     model = smp.DeepLabV3Plus(\n#         encoder_name = \"resnet34\",\n#         encoder_weights = \"imagenet\",\n#         in_channels = 1,\n#         classes = 3,\n#         activation = None,\n#     )\n\n    model = smp.DeepLabV3Plus(\n        encoder_name = \"efficientnet-b0\",\n        encoder_weights = \"imagenet\",\n        in_channels = 1,\n        classes = 3,\n        activation = None,\n    )\n\n#     model = smp.UnetPlusPlus(\n#         encoder_name = \"resnet34\",\n#         encoder_weights = \"imagenet\",\n#         in_channels = 1,\n#         classes = 3,\n#         activation = None,\n#         decoder_attention_type = \"scse\",\n#     )\n\n    model.to(DEVICE)\n    \n#     criterion = smp.losses.FocalLoss(mode = 'multiclass', normalized = True)\n#     criterion = smp.losses.FocalLoss(mode = 'multiclass')\n#     criterion = smp.losses.DiceLoss(mode = 'multiclass')\n    DiceLoss = smp.losses.DiceLoss(mode = 'multiclass').to(DEVICE)\n    FocalLoss = smp.losses.FocalLoss(mode = 'multiclass').to(DEVICE)\n    NormFocalLoss = smp.losses.FocalLoss(mode = 'multiclass', normalized = True).to(DEVICE)\n#     JaccardLoss = smp.losses.JaccardLoss(mode = 'multiclass')\n#     optimizer = torch.optim.AdamW(model.parameters(), lr = LEARNING_RATE) # Maybe add weight decay\n    optimizer = torch.optim.AdamW(model.parameters(), lr = LEARNING_RATE) # Maybe add weight decay\n#     lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer = optimizer, gamma = LEARNING_RATE_DECAY)\n    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer)\n#     lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer = optimizer, T_max = 3)\n\n    criterions = {}\n    \n#     criterions[0] = smp.losses.FocalLoss(mode = 'multiclass')\n#     criterions[1] = smp.losses.DiceLoss(mode = 'multiclass')\n    \n    start_epoch = 1\n\nprint(sum(p.numel() for p in model.parameters() if p.requires_grad))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculateIoU(prediction, mask):\n    prediction = torch.argmax(prediction, dim = 1).cpu().detach().numpy()\n    mask = mask.cpu().detach().numpy()\n    mask1 = np.where(mask == 1, 1, 0).astype(np.uint8)\n    mask2 = np.where(mask == 2, 1, 0).astype(np.uint8)\n\n    kernel = np.ones((3,3))\n\n    erosion1 = cv2.erode(mask1, kernel, iterations = 1) \n    dilation1 = cv2.dilate(mask1, kernel, iterations = 1)\n\n    erosion2 = cv2.erode(mask2, kernel, iterations = 1) \n    dilation2= cv2.dilate(mask2, kernel, iterations = 1)\n    \n    erodedMask = np.zeros(mask.shape, dtype = np.uint8)\n    erodedMask[erosion1 == 1] = 1\n    erodedMask[erosion2 == 1] = 2\n    \n    dilatedMask = np.zeros(mask.shape, dtype = np.uint8)\n    dilatedMask[dilation1 == 1] = 1\n    dilatedMask[dilation2 == 1] = 2    \n    \n    intersection = np.where(np.logical_and(dilatedMask == prediction, dilatedMask != 0), 1, 0)        \n    intersectionCount = np.count_nonzero(intersection)\n\n    union = np.where(np.logical_or(erodedMask != 0, prediction != 0), 1, 0)\n    unionCount = np.count_nonzero(union)\n    \n    if(unionCount == 0): score = 1\n    else: score = intersectionCount / unionCount\n    \n    return score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision\nimport wandb\n\ndef calc_loss(outputs, masks):\n    return DiceLoss(outputs, masks) + (FocalLoss(outputs, masks) * 7 + NormFocalLoss(outputs, masks) * 1e5 * 7) / 2.0\n\ndef TrainEpoch(model, loader, run, epoch):\n    print(\"Training:\")\n    \n    model.train()\n\n    train_loss_sum = 0.0\n    dice_loss_sum = 0.0\n    loss_count = 0\n    cur_loss = 0.0\n    cur_dice_loss = 0.0\n\n    for i, data in enumerate(loader, 1):\n        inputs, masks = data\n\n        inputs = preprocess(inputs).to(DEVICE).unsqueeze(1)\n        masks = masks.to(DEVICE)\n\n        optimizer.zero_grad(set_to_none = True)\n\n        outputs = model(inputs)\n        \n        loss = calc_loss(outputs, masks)\n        dice_loss = DiceLoss(outputs, masks)\n\n        loss.backward()\n        optimizer.step()\n\n        cur_loss += loss.item()\n        train_loss_sum += loss.item()\n        loss_count += 1\n        \n        cur_dice_loss += dice_loss.item()\n        dice_loss_sum += dice_loss.item()\n\n        if(i % TRAIN_BATCH_COUNT == 0):\n            average_loss = cur_loss / TRAIN_BATCH_COUNT\n            average_dice_loss = cur_dice_loss / TRAIN_BATCH_COUNT\n            print(\"    Epoch: %d, Batch Count: [%d, %d], Average Loss: %.16f, Average Dice Loss %.16f\" % (epoch, i, len(loader), average_loss, average_dice_loss))\n            run.log({\"Training Loss\" : average_loss})\n            run.log({\"Training Dice Loss\" : average_dice_loss})\n            cur_loss = 0\n            cur_dice_loss = 0\n\n    run.log({\"Training Loss Average\" : train_loss_sum / loss_count})\n    run.log({\"Training Dice Loss Average\" : dice_loss_sum / loss_count})\n        \ndef ValidEpoch(model, loader, dataset, run, epoch):\n    print(\"Validation:\")\n            \n    model.eval()\n\n    with torch.no_grad():\n        valid_loss_sum = 0.0\n        dice_loss_sum = 0.0\n\n        loss_count = 0\n        cur_loss = 0.0\n        cur_dice_loss = 0.0\n\n        for i, data in enumerate(loader, 1):\n            inputs, masks = data\n\n            inputs = preprocess(inputs).to(DEVICE).unsqueeze(1)\n            masks = masks.to(DEVICE)\n\n            outputs = model(inputs)\n        \n            loss = calc_loss(outputs, masks)\n            dice_loss = DiceLoss(outputs, masks)\n\n            cur_loss += loss.item()\n            valid_loss_sum += loss.item()\n            \n            cur_dice_loss += dice_loss.item()\n            dice_loss_sum += dice_loss.item()\n            \n            loss_count += 1\n\n            if(i % VALID_BATCH_COUNT == 0):\n                average_loss = cur_loss / VALID_BATCH_COUNT\n                average_dice_loss = cur_dice_loss / VALID_BATCH_COUNT\n                print(\"    Epoch: %d, Batch Count: [%d, %d], Average Loss: %.16f, Average Dice Loss %.16f\" % (epoch, i, len(loader), average_loss, average_dice_loss))\n                run.log({\"Validation Loss\" : average_loss})\n                run.log({\"Validation Dice Loss\" : average_dice_loss})\n                cur_loss = 0\n                cur_dice_loss = 0\n\n        run.log({\"Validation Loss Average\" : valid_loss_sum / loss_count})\n        run.log({\"Validation Dice Loss Average\" : dice_loss_sum / loss_count})\n\n        table = wandb.Table(columns = [\"id\", \"image\", \"mask\", \"prediction\", \"difference\", \"correct (green)\", \"couldn\\'t predict (blue)\", \"predicted wrongly (red)\"])\n\n        for i in range(0, VALID_IMAGE_COUNT):\n            image, mask = dataset[i]\n\n            tensr = torch.tensor(preprocess(image)).to(DEVICE).unsqueeze(0).unsqueeze(0)\n\n            output = model(tensr)\n\n            output = torch.argmax(output, dim = 1)\n\n            prediction = output.squeeze(0).cpu().detach().numpy()\n\n            image_fixed = image.astype('int64')\n            mask_fixed = fix_image_colors(mask).astype('int64')\n            prediction_fixed = fix_image_colors(prediction).astype('int64')\n            difference = get_difference(prediction, mask)\n\n            diff = difference['diff']\n            green = difference['green']\n            blue = difference['blue']\n            red = difference['red']\n\n            table.add_data(i + 1, wandb.Image(image_fixed), wandb.Image(mask_fixed), wandb.Image(prediction_fixed), wandb.Image(diff), green, blue, red)\n            \n        IoUSum = 0.0\n            \n        for image, mask in datasets['valid']:\n            tensr = torch.tensor(preprocess(image)).to(DEVICE).unsqueeze(0).unsqueeze(0)\n            \n            output = model(tensr)\n            \n            IoUSum += calculateIoU(output, mask)\n            \n        run.log({\"IoU Score\" : IoUSum / len(datasets['valid'])})\n\n        run.log({(\"Validation Images %d\" % epoch) : table})\n        \n        return valid_loss_sum / loss_count","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"wandb_api_key\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wandb login $api_key","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.backends.cudnn.benchmark = True\ntorch.autograd.set_detect_anomaly(False)\ntorch.autograd.profiler.emit_nvtx(False)\ntorch.autograd.profiler.profile(False)\n\nrun = wandb.init(project = PROJECT_NAME)\n\nfor epoch in range(start_epoch, start_epoch + EPOCH_COUNT):\n    for phase in ['train', 'valid']:\n        if(phase == 'train'):\n            TrainEpoch(model, loaders['train'], run, epoch)\n        else:\n            loss = ValidEpoch(model, loaders['valid'], datasets['valid_images'], run, epoch)\n            \n    lr_scheduler.step(loss)\n    \n    if(epoch % SAVE_PER_BATCH == 0):\n        checkpoint = { \n            'epoch': epoch,\n            'model': model,\n            'criterions' : [DiceLoss, FocalLoss, NormFocalLoss],\n            'optimizer': optimizer,\n            'lr_scheduler': lr_scheduler\n        }\n        \n        torch.save(checkpoint, (\"./model_checkpoint\" + str(epoch) + \".pth\"))\n        wandb.save(\"./model_checkpoint\" + str(epoch) + \".pth\")\n\nrun.finish()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\nDiceLoss = smp.losses.DiceLoss(mode = 'multiclass')\n\nwith torch.no_grad():\n    dice_loss_sum = 0.0\n\n    loss_count = 0\n    cur_dice_loss = 0.0\n\n    for i, data in enumerate(datasets['valid'], 1):\n        inputs, masks = data\n\n        inputs = torch.tensor(preprocess(inputs)).to(DEVICE).unsqueeze(0).unsqueeze(0)\n        masks = torch.tensor(masks).to(DEVICE).unsqueeze(0)\n\n        outputs = model(inputs)\n\n        dice_loss = DiceLoss(outputs, masks)\n\n        cur_dice_loss += dice_loss.item()\n        dice_loss_sum += dice_loss.item()\n\n        loss_count += 1\n        \n    print(dice_loss_sum / loss_count)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = [[], [], [], []]\n\nmodel.eval()\n\nfor i in range(0, VALID_IMAGE_COUNT):\n    image, mask = datasets['valid_images'][i]\n\n    tensr = torch.tensor(preprocess(image)).to(DEVICE).unsqueeze(0).unsqueeze(0)\n    \n    output = model(tensr)\n\n    output = torch.argmax(output, dim = 1)\n\n    prediction = output.squeeze(0).cpu().detach().numpy()\n\n    image_fixed = image.astype('int64')\n    mask_fixed = fix_image_colors(mask).astype('int64')\n    prediction_fixed = fix_image_colors(prediction).astype('int64')\n    difference = get_difference(prediction, mask)\n    \n    images[0].append(image_fixed)\n    images[1].append(mask_fixed)\n    images[2].append(prediction_fixed)\n    images[3].append(difference['diff'])\n    \nshow_all(images, ['Dicom', 'Mask', 'Prediction', 'Difference'])","metadata":{},"execution_count":null,"outputs":[]}]}